{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c922ff95-a39e-4df7-857c-316b6a7e5e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def basic_eda(df):\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(\"First 5 rows:\")\n",
    "    display(df.limit(5))\n",
    "    \n",
    "    row_count = df.count()\n",
    "    print(f\"Number of rows: {row_count}\")\n",
    "    \n",
    "    # Select only columns with DoubleType for statistics\n",
    "    double_cols = [\n",
    "        field.name\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(field.dataType, DoubleType)\n",
    "    ]\n",
    "    if double_cols:\n",
    "        print(\"Summary statistics for double columns:\")\n",
    "        display(df.select(double_cols).describe())\n",
    "    else:\n",
    "        print(\"No double type columns found.\")\n",
    "    \n",
    "    print(\"Null values in each column:\")\n",
    "    null_counts = df.select([\n",
    "        col(c).isNull().cast(\"int\").alias(c) for c in df.columns\n",
    "    ]).groupBy().sum()\n",
    "    display(null_counts)\n",
    "    \n",
    "    print(\"Duplicate rows count:\")\n",
    "    duplicate_rows_count = df.count() - df.dropDuplicates().count()\n",
    "    print(duplicate_rows_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8679ab0-b349-45ff-a549-2a961ccefed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, split\n",
    "\n",
    "def fill_missing_marca(df):\n",
    "    # Extract the first word from the nome_produto column\n",
    "    df = df.withColumn(\"first_word\", split(col(\"nome_produto\"), \" \").getItem(0))\n",
    "    \n",
    "    # Fill missing values in the marca column with the first word from nome_produto\n",
    "    df = df.withColumn(\"marca\", when(col(\"marca\").isNull(), col(\"first_word\")).otherwise(col(\"marca\")))\n",
    "    \n",
    "    # Drop the temporary first_word column\n",
    "    df = df.drop(\"first_word\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292be504-9224-489b-8be3-75f7017ec747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1335c546-de3c-4dfe-9d70-373b88d595c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df_clientes = spark.read.format(\"delta\").table(\"workspace.default.clientes\")\n",
    "basic_eda(df_clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7848ca57-d6a6-489c-b5e4-226446b4bd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df_produtos = spark.read.format(\"delta\").table(\"workspace.default.produtos\")\n",
    "basic_eda(df_produtos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fc935f-22c0-4a43-9537-65cbf30b8305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df_vendas = spark.read.format(\"delta\").table(\"workspace.default.vendas\")\n",
    "basic_eda(df_vendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02a5b80-5604-4a9d-b745-a5b08d060747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df_produtos = spark.read.format(\"delta\").table(\"workspace.default.produtos\")\n",
    "df_produtos = fill_missing_marca(df_produtos)\n",
    "display(df_produtos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dac157-edf8-4a86-9d64-3b7c391a1df6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756656432149}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_vendas.join(df_clientes, \"id_cliente\", \"inner\").join(df_produtos, \"id_produto\", \"inner\")\n",
    "df_joined = df_joined.select('id_venda', 'canal_venda', 'valor', 'data_venda', 'id_produto', 'nome_produto', 'descricao', 'marca', 'id_cliente', 'nome', 'email', 'telefone', 'cidade', 'estado', 'data_cadastro')\n",
    "\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbeec2e-77de-4c69-aed6-95cc898ae6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "basic_eda(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1cc614-9fa1-4694-883d-e97c06303068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = df_clientes.toPandas()\n",
    "num_cols = pdf.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08f5022-5a36-4190-bde0-20059b97d366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def graphical_eda(df, num_cols=None, cat_cols=None):\n",
    "    # Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Plot 1: Distribution of numerical columns\n",
    "    if num_cols is None:\n",
    "        num_cols = ['valor']\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        sns.histplot(pdf[col].dropna(), kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot 2: Count plot for categorical columns\n",
    "    if cat_cols is None:\n",
    "        cat_cols = ['canal_venda', 'marca', 'estado']\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(15, 20))\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        sns.countplot(y=pdf[col].dropna(), order=pdf[col].value_counts().index)\n",
    "        plt.title(f'Count plot of {col}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot 3: Correlation heatmap for numerical columns\n",
    "    num_cols_ttl = ['id_venda', 'canal_venda', 'valor', 'data_venda', 'id_produto', 'nome_produto', 'descricao', 'marca', 'id_cliente', 'nome', 'email', 'telefone', 'cidade', 'estado', 'data_cadastro']\n",
    "    # pdf.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(num_cols_ttl) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        corr = pdf[num_cols_ttl].corr()\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Correlation Heatmap')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "graphical_eda(df_joined, num_cols=['valor'], cat_cols=['canal_venda', 'marca', 'estado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff160d0-1589-41d1-8db6-8e25d21e6bf6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756657996180}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, col, round\n",
    "\n",
    "total_count = df_joined.count()\n",
    "total_sum_valor = df_joined.agg(sum('valor')).collect()[0][0]\n",
    "\n",
    "df_prod_agg = df_joined.groupBy('nome_produto').agg(\n",
    "    count('id_venda').alias('count_vendas'),\n",
    "    round(sum('valor'), 2).alias('sum_valor'),\n",
    "    round(avg('valor'), 2).alias('avg_valor')\n",
    "    ).withColumn(\n",
    "        'percent_count_vendas', round((col('count_vendas') / total_count) * 100, 2)\n",
    "    ).withColumn(\n",
    "        'percent_sum_valor', round((col('sum_valor') / total_sum_valor) * 100, 2)\n",
    "    )\n",
    "    \n",
    "    \n",
    "df_prod_agg = df_prod_agg.select('nome_produto','count_vendas','percent_count_vendas','sum_valor','percent_sum_valor','avg_valor').orderBy('count_vendas', ascending=False)\n",
    "\n",
    "df_prod_agg.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fb4631-f36e-414a-8b58-16a9be5aba5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, col, round, avg, countDistinct\n",
    "\n",
    "total_count = df_joined.agg(countDistinct('id_venda')).collect()[0][0]\n",
    "total_sum_valor = df_joined.agg(sum('valor')).collect()[0][0]\n",
    "\n",
    "df_canal_agg = df_joined.groupBy('canal_venda').agg(\n",
    "    count('id_venda').alias('count_vendas'),\n",
    "    round(sum('valor'), 2).alias('sum_valor'),\n",
    "    round(avg('valor'), 2).alias('avg_valor')\n",
    ").withColumn(\n",
    "    'percent_count_vendas', round((col('count_vendas') / total_count) * 100, 2)\n",
    ").withColumn(\n",
    "    'percent_sum_valor', round((col('sum_valor') / total_sum_valor) * 100, 2)\n",
    ")\n",
    "\n",
    "df_canal_agg = df_canal_agg.select(\n",
    "    'canal_venda', 'count_vendas', 'percent_count_vendas', \n",
    "    'sum_valor', 'percent_sum_valor', 'avg_valor'\n",
    ").orderBy('count_vendas', ascending=False)\n",
    "\n",
    "df_canal_agg.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eef02f3-b0de-436b-8b05-2bed8560b604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, count, sum, avg, col, round\n",
    "\n",
    "# Extract year and month from data_venda\n",
    "df_seasonal = df_joined.withColumn(\"ano\", year(col(\"data_venda\"))) \\\n",
    "    .withColumn(\"mes\", month(col(\"data_venda\")))\n",
    "\n",
    "# Calculate total counts and sums for percent calculations\n",
    "total_count = df_seasonal.count()\n",
    "total_sum_valor = df_seasonal.agg(sum('valor')).collect()[0][0]\n",
    "\n",
    "# Aggregate by year and month\n",
    "df_sazonal_agg = df_seasonal.groupBy(\"ano\", \"mes\").agg(\n",
    "    count(\"id_venda\").alias(\"count_vendas\"),\n",
    "    round(sum(\"valor\"), 2).alias(\"sum_valor\"),\n",
    "    round(avg(\"valor\"), 2).alias(\"avg_valor\")\n",
    ").withColumn(\n",
    "    \"percent_count_vendas\", round((col(\"count_vendas\") / total_count) * 100, 2)\n",
    ").withColumn(\n",
    "    \"percent_sum_valor\", round((col(\"sum_valor\") / total_sum_valor) * 100, 2)\n",
    ")\n",
    "\n",
    "df_sazonal_agg = df_sazonal_agg.select(\n",
    "    \"ano\", \"mes\", \"count_vendas\", \"percent_count_vendas\",\n",
    "    \"sum_valor\", \"percent_sum_valor\", \"avg_valor\"\n",
    ").orderBy(\"ano\", \"mes\")\n",
    "\n",
    "display(df_sazonal_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d593747a-bb79-4983-baac-9f5d56181dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "pdf_sazonal_agg = df_sazonal_agg.toPandas()\n",
    "\n",
    "# Plotting the seasonal aggregation data as a bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(data=pdf_sazonal_agg, x='mes', y='sum_valor', hue='ano')\n",
    "plt.title('Monthly Sales Value by Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales Value')\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c194e7-a5bf-46a7-8289-385caa32737a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, col, sum\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract year and month from data_venda\n",
    "df_seasonal = df_joined.withColumn(\"ano\", year(col(\"data_venda\"))) \\\n",
    "    .withColumn(\"mes\", month(col(\"data_venda\")))\n",
    "\n",
    "# Aggregate by year and month\n",
    "df_sazonal_agg = df_seasonal.groupBy(\"ano\", \"mes\").agg(\n",
    "    sum(\"valor\").alias(\"sum_valor\")\n",
    ").orderBy(\"ano\", \"mes\")\n",
    "\n",
    "# Prepare features and label\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"ano\", \"mes\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_features = assembler.transform(df_sazonal_agg).select(\"features\", \"sum_valor\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"sum_valor\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"sum_valor\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "\n",
    "# Display predictions\n",
    "display(predictions)\n",
    "\n",
    "# Predict future sales for the next quarter\n",
    "current_year = datetime.now().year\n",
    "current_month = datetime.now().month\n",
    "future_dates = [(current_year, current_month + i) for i in range(1, 4)]\n",
    "future_df = pd.DataFrame(future_dates, columns=[\"ano\", \"mes\"])\n",
    "\n",
    "# Convert to Spark DataFrame using the existing Spark context\n",
    "future_spark_df = spark.createDataFrame(future_df)\n",
    "\n",
    "# Prepare features for prediction\n",
    "future_features = assembler.transform(future_spark_df).select(\"features\")\n",
    "\n",
    "# Make predictions for the next quarter\n",
    "future_predictions = lr_model.transform(future_features)\n",
    "\n",
    "# Display future predictions\n",
    "display(future_predictions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gerando_Falcoes_v0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
