{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e96bfb-b9fa-475e-8346-e5a703bfa8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Desafio Gerando Falções\n",
    "## ETL\n",
    "\n",
    "Este notebook o processo de ETL para a extração e processamento dos dados fornecidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbce5874-718d-48c1-88eb-0e5787cebe47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2aea4a0-5ef9-4caa-9cfa-e84fd0006975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, DoubleType, DateType, TimestampType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125bc669-36f9-4640-9b0b-ffcb60be7a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções Utilitárias\n",
    "\n",
    "Abaixo, encontam-se as funções com as rotinas necessárias para cada etapa do processo de ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74c0dbb-22e9-4a3c-9b08-64a898da701b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ac1ade-85b6-475b-94a4-2c47503b4ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def obtem_dados(table_name, csv_path, database=\"default\"):\n",
    "    try:\n",
    "        # Check if the table already exists\n",
    "        df = spark.read.format(\"delta\").table(f\"{database}.{table_name}\")\n",
    "    except AnalysisException:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Write the DataFrame to a Delta table\n",
    "        df.write.format(\"delta\").saveAsTable(f\"{database}.{table_name}\")\n",
    "        print(f\"Table {database}.{table_name} created from {csv_path}.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af64b608-ce24-42d6-a932-746f0f0a8d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348c4032-9741-431f-bce7-0b373a46184a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cria_df_unificado(vendas_df, produtos_df, clientes_df):\n",
    "    # Cria dataframe unificado a partir \n",
    "    df_unificado = (\n",
    "        vendas_df\n",
    "        .join(produtos_df, \"id_produto\", \"inner\")\n",
    "        .join(clientes_df, \"id_cliente\", \"inner\")\n",
    "    )\n",
    "\n",
    "    # Adiciona coluna ano-mes, com o ano e o mês da data de venda\n",
    "    df_unificado = df_unificado.withColumn(\"ano_mes_venda\", F.date_format(F.col('data_venda'), \"yyyy-MM\"))\n",
    "\n",
    "    # Re-ordena e seleciona somente as colunas relevantes\n",
    "    # Exclusão das colunas marca e cidade, como demonstrado na análise exploratória\n",
    "    df_unificado = df_unificado.select(\n",
    "        'id_venda', 'canal_venda', 'valor', 'data_venda', 'ano_mes_venda',\n",
    "        'id_produto', 'nome_produto', 'descricao', \n",
    "        'id_cliente', 'nome', 'email', 'telefone', 'estado', 'data_cadastro'\n",
    "    )\n",
    "\n",
    "    return df_unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b96219-16c7-49f8-a0fd-981b86b2a7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição do schema para a UDF\n",
    "schema_cat_brand_model = StructType([\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"marca\", StringType(), True),\n",
    "    StructField(\"modelo\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Função UDF para extrair categoria, marca e modelo do nome do produto, com base em uma lista de BRANDS\n",
    "@F.udf(returnType=schema_cat_brand_model)\n",
    "def extract_cat_brand_model(nome_produto: str):\n",
    "    if not nome_produto:\n",
    "        return {\"categoria\": None, \"marca\": None, \"modelo\": None}\n",
    "\n",
    "    toks = nome_produto.strip().split()\n",
    "    marca_idx = None\n",
    "    marca_name = None\n",
    "\n",
    "    # Identifica a marca no nome do produto, com base na lista BRANDS\n",
    "    for i, tok in enumerate(toks):\n",
    "        tok_clean = re.sub(r\"[\\-\\+]\", \" \", tok)\n",
    "        for brand in BRANDS:\n",
    "            if tok.lower() == brand.lower() or tok_clean.lower() == brand.lower():\n",
    "                marca_idx = i\n",
    "                marca_name = brand\n",
    "                break\n",
    "        if marca_idx is not None:\n",
    "            break\n",
    "\n",
    "    # Extrai categoria e modelo com base na posição da marca\n",
    "    if marca_idx is not None:\n",
    "        categoria = \" \".join(toks[:marca_idx]).strip() or None\n",
    "        modelo = \" \".join(toks[marca_idx+1:]).strip() or None\n",
    "        return {\"categoria\": categoria, \"marca\": marca_name, \"modelo\": modelo}\n",
    "    else:\n",
    "        categoria = toks[0]\n",
    "        modelo = \" \".join(toks[1:]).strip() or None\n",
    "        return {\"categoria\": categoria, \"marca\": None, \"modelo\": modelo}\n",
    "\n",
    "# Função para substituir valores vazios por nulos\n",
    "def null_if_empty(col):\n",
    "    return F.when(F.trim(col) == \"\", None).otherwise(col)\n",
    "\n",
    "# Função para enriquecer o DataFrame de produtos\n",
    "def enriquece_produtos(df, BRANDS):\n",
    "    \"\"\"\n",
    "    Enriquecer DataFrame de produtos com categoria, marca, modelo, capacidade, dimensões, potência e memória.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df : PySpark DataFrame\n",
    "            DataFrame de entrada com pelo menos as colunas 'nome_produto' e 'descricao'.\n",
    "    \n",
    "    Retorna:\n",
    "        PySpark DataFrame com colunas adicionadas:\n",
    "        ['categoria', 'marca', 'modelo', 'capacidade', 'dimensao', 'potencia']\n",
    "    \"\"\"\n",
    "    # Extrai categoria, marca e modelo do nome do produto\n",
    "    df_enriquecido = df.withColumn(\n",
    "        \"cat_brand_model\",\n",
    "        extract_cat_brand_model(F.col(\"nome_produto\"))\n",
    "    ).withColumn(\"categoria\", F.col(\"cat_brand_model.categoria\")) \\\n",
    "     .withColumn(\"marca\", F.col(\"cat_brand_model.marca\")) \\\n",
    "     .withColumn(\"modelo\", F.col(\"cat_brand_model.modelo\")) \\\n",
    "     .drop(\"cat_brand_model\")\n",
    "\n",
    "    # Cria um padrão regex para corresponder a qualquer nome de marca\n",
    "    brands_pattern = '|'.join(BRANDS)\n",
    "\n",
    "    # Atualiza o DataFrame para remover nomes de marcas da coluna 'nome_produto'\n",
    "    df_enriquecido = df_enriquecido.withColumn(\n",
    "        \"nome_produto_cleaned\",\n",
    "        F.regexp_replace(F.col(\"nome_produto\"), f\"\\\\b({brands_pattern})\\\\b\", \"\")\n",
    "    )\n",
    "    \n",
    "    # Atualiza a coluna 'categoria' com base no nome do produto limpo\n",
    "    # A Categoria consiste em strings de até 2 palavras (3 se conter stop-words) que descrevem em termos gerais, a categoria do produto\n",
    "    df_enriquecido = df_enriquecido.withColumn(\n",
    "        \"categoria\",\n",
    "        F.when(\n",
    "            (F.size(F.split(F.col(\"nome_produto_cleaned\"), \" \")) > 3) & \n",
    "            (F.array_contains(F.array(F.lit(\"de\"), F.lit(\"da\"), F.lit(\"do\"), F.lit(\"das\"), F.lit(\"dos\"), F.lit(\"a\"), F.lit(\"o\"), F.lit(\"e\")), \n",
    "                            F.split(F.col(\"nome_produto_cleaned\"), \" \")[1])),\n",
    "            F.concat_ws(\" \", F.slice(F.split(F.col(\"nome_produto_cleaned\"), \" \"), 1, 3))\n",
    "        ).when(\n",
    "            F.size(F.split(F.col(\"nome_produto_cleaned\"), \" \")) > 2,\n",
    "            F.concat_ws(\" \", F.slice(F.split(F.col(\"nome_produto_cleaned\"), \" \"), 1, 2))\n",
    "        ).otherwise(F.col(\"nome_produto_cleaned\"))\n",
    "    )\n",
    "\n",
    "    # Extrai capacidade em litros da descrição ou nome do produto\n",
    "    df_enriquecido = df_enriquecido.withColumn(\n",
    "        \"capacidade\",\n",
    "        F.coalesce(\n",
    "            null_if_empty(F.regexp_extract(F.col(\"descricao\"), r\"(\\d+(?:,\\d+)?\\s?[lL])\", 1)),\n",
    "            null_if_empty(F.regexp_extract(F.col(\"nome_produto\"), r\"(\\d+(?:,\\d+)?\\s?[lL])\", 1))\n",
    "        )\n",
    "    ).withColumn(\n",
    "        # Extrai dimensões da descrição ou nome do produto\n",
    "        \"dimensao\",\n",
    "        F.coalesce(\n",
    "            null_if_empty(F.regexp_extract(F.col(\"descricao\"), r\"(\\d+(?:,\\d+)?\\s?cm)\", 1)),\n",
    "            null_if_empty(F.regexp_extract(F.col(\"nome_produto\"), r\"(\\d+(?:,\\d+)?\\s?cm)\", 1)),\n",
    "            null_if_empty(F.regexp_extract(F.col(\"descricao\"), r\"(\\d+(?:\\.\\d+)?\\s*(\\\"|”|''|pol))\", 1)),\n",
    "            null_if_empty(F.regexp_extract(F.col(\"nome_produto\"), r\"(\\d+(?:\\.\\d+)?\\s*(\\\"|”|''|pol))\", 1))\n",
    "        )\n",
    "    ).withColumn(\n",
    "        # Extrai potência em watts da descrição ou nome do produto\n",
    "        \"potencia\",\n",
    "        F.coalesce(\n",
    "            null_if_empty(F.regexp_extract(F.col(\"descricao\"), r\"(\\d+(?:,\\d+)?\\s?W)\", 1)),\n",
    "            null_if_empty(F.regexp_extract(F.col(\"nome_produto\"), r\"(\\d+(?:,\\d+)?\\s?W)\", 1))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Seleciona e re-ordena colunas\n",
    "    df_enriquecido = df_enriquecido.select(\n",
    "        'id_venda', 'canal_venda', 'valor', 'data_venda', 'ano_mes_venda',\n",
    "        'id_produto', 'nome_produto', 'descricao', 'categoria', 'marca', 'modelo', 'capacidade', 'dimensao', 'potencia',\n",
    "        'id_cliente', 'nome', 'email', 'telefone', 'estado', 'data_cadastro'\n",
    "    )\n",
    "\n",
    "    return df_enriquecido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb4e1ae-6d9f-45f4-8572-be33741db7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd6282c-e287-47e0-a5dc-6fe05314a792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cria_tabela_delta(df, table_name, database=\"default\"):\n",
    "    # Escreve o DataFrame em uma tabela Delta\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{database}.{table_name}\")\n",
    "    print(f\"Tabela {database}.{table_name} criada ou sobrescrita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01374a93-c01c-485f-acf5-2b3252f6f67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def exibe_df(df, nome_df):    \n",
    "    # Exibe informações úteis\n",
    "    print(f\"\\nNome do DataFrame: {nome_df}\")\n",
    "    print(f\"Número de linhas: {df.count()}\")\n",
    "    print(f\"Número de colunas: {len(df.columns)}\")\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Exibe o DataFrame\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a735e757-7478-4c22-88b0-bfae58ae1b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execução da ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34e56c6-39c8-4707-9c23-0d6f6d647a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# CONFIGURAÇÃO DE VARIÁVEIS\n",
    "#############################\n",
    "\n",
    "# Define paths\n",
    "clientes_csv_path = \"/Users/bentofreitas/Documents/clientes.csv\"\n",
    "produtos_csv_path = \"/Users/bentofreitas/Documents/produtos.csv\"\n",
    "vendas_csv_path = \"/Users/bentofreitas/Documents/vendas.csv\"\n",
    "\n",
    "# Lista de marcas conhecidas\n",
    "BRANDS = [\"Mondial\", \"Dell\", \"Samsung\", \"LG\", \"Nespresso\", \"Arno\", \"Electrolux\", \"Brastemp\", \n",
    "          \"Black+Decker\", \"HP\", \"Lorenzetti\", \"Fischer\", \"Philips Walita\", \"Redragon\", \n",
    "          \"Logitech\", \"Xiaomi\", \"Apple\", \"Motorola\", \"Acer\", \"ThunderX3\", \"Fifine\", \n",
    "          \"Lenovo\", \"JBL\", \"Kingston\", \"Seagate\", \"TP-Link\", \"Clamper\", \"Britânia\", \"WAP\"]\n",
    "\n",
    "#############################\n",
    "# EXTRAÇÃO DOS DADOS\n",
    "#############################\n",
    "\n",
    "# Cria tabelas Delta e obtém DataFrames\n",
    "# Extração dos dados dos arquivos CSV e criação das tabelas Delta correspondentes\n",
    "clientes_df = obtem_dados(\"clientes\", clientes_csv_path)\n",
    "produtos_df = obtem_dados(\"produtos\", produtos_csv_path)\n",
    "vendas_df = obtem_dados(\"vendas\", vendas_csv_path)\n",
    "\n",
    "#############################\n",
    "# TRANSFORMAÇÃO DOS DADOS\n",
    "#############################\n",
    "\n",
    "# Unificação dos DataFrames de vendas, produtos e clientes\n",
    "df_unificado = cria_df_unificado(vendas_df, produtos_df, clientes_df)\n",
    "\n",
    "# Enriquecimento do DataFrame unificado com informações adicionais dos produtos\n",
    "df_enriquecido = enriquece_produtos(df_unificado, BRANDS)\n",
    "\n",
    "#############################\n",
    "# CARREGAMENTO DOS DADOS\n",
    "#############################\n",
    "\n",
    "# Exibição do DataFrame enriquecido para verificação\n",
    "exibe_df(df_enriquecido, \"base_unificada\")\n",
    "\n",
    "# Grava resultados em tabelas Delta\n",
    "cria_tabela_delta(df_enriquecido, \"Base_Unificada_GF\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gerando_Falcoes_ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
